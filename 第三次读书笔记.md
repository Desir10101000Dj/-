# 第三次读书笔记 #
---
## 2.1经验误差与过拟合 ##
 错误率：分类错误的样本占样本总数的比例。  E = a／m

    精度：1 - a／m

    训练误差／经验误差：训练集上的误差

    泛化误差：新样本上的误差。
## 2.2评估方法 ##
 区分数据集，把数据集分为两部分，一部分是训练集S另外一部分是测试集T
### 2.2.1 留出法 ###
    一个数据集分成互斥的两部分，分别作为训练集和测试集。可以通过分层采样的方法，保证两个数据集的数据分布一致。

    一般会多次区分数据集得到多次结果，然后得出平均值最为最终结果。
    
代码模拟：

    from sklearn.model_selection import train_test_split
    from sklearn import datasets
    iris = datasets.load_iris()
    X_train,X_test,y_train,y_test= train_test_split(iris.data,iris.target,test_size=0.3,random_state=0,stratify=iris.target)

    print("X_train=",X_train)
    print("X_test=",X_test)
    print("y_train=",y_train)
    print("y_test=",y_test)
 ![留出法](https://github.com/Desir10101000Dj/--/blob/master/image/%E7%95%99%E5%87%BA.PNG?raw=true)

### 2.2.2 交叉验证法 ###
    从数据集D中通过分层采样得到k个大小相似的互斥子集。

    每次选择k-1个子集的并集作为训练集，另外一个作为测试集。最终可以进行k次训练和测试。最终返回k次的平均值。这种方法通常也被成为“k折交叉验证”。k的最常用取值是10.

    如果在每次划分k个不同子集的时候，根据不同的划分规则，可以得到不同的k个子集。结合之前的留出法，就可以产生p次k折交叉验证。最常见的是10次10折交叉验证。

   

代码模拟：


                               from sklearn.model_selection import KFold
                               import numpy as np
                               X = np.random.rand(9,4)
                               y = np.array([1,1,0,0,1,1,0,0,1])
                               folder = KFold(n_splits=3,random_state=0,shuffle=True)
                               for train_index,test_index in folder.split(X,y):
                                   print("Train Index:",train_index)
                                   print("Test Index:",test_index)
                                   print("X_train:",X[train_index])
                                   print("X_test:",X[test_index])
                                   print("")
                                   
  ![交叉验证法](https://github.com/Desir10101000Dj/--/blob/master/image/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81.PNG?raw=true)
  
              
    当k等于样本个数时，变成留一法。
代码模拟：


                               from sklearn.model_selection import LeaveOneOut
                               import numpy as np
                               X = np.array([[1,2,3,4],
                                            [11,12,13,14],
                                            [21,22,23,24],
                                            [31,32,33,34]])
                               y = np.array([1,1,0,0])
                               loo = LeaveOneOut()
                               for train_index,test_index in loo.split(X):
                                   print("Levave-One-Out Train Index:",train_index)
                                   print("Levave-One-Out Test Index:",test_index)
                                   print("Levave-One-Out X_train:",X[train_index])
                                   print("Levave-One-Out X_test:",X[test_index])
                                   print('\n')
![留一法](https://github.com/Desir10101000Dj/--/blob/master/image/%E7%95%99%E4%B8%80%E6%B3%95.PNG?raw=true)

    

###  2.2.3 自助法 ###
    从数据集D中取出放回m次形成一个新的数据集D'，D'与D相同大小。但是会存在部分数据重复。大概在D中会有

    （1−1/m）m个数据不会被放到D'中。大约占0.368。然后用D'作训练集，D作测试集。

    自助法在数据集较小，难以有效划分训练／测试集的时候很有用。但是自助法改变了初始数据集的分布，会引入估计偏差。所以当数据足够时，还是使用留出法和交叉验证法。
代码模拟：


                               import numpy as np
                               import pandas as pd
                               import random
                               data = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))
                               data['y'] = [random.choice([0,1]) for i in range(10)]
                               train = data.sample(frac=1.0,replace=True) # 有放回随机采样
                               test = data.loc[data.index.difference(train.index)].copy() # 将未采样的样本作为测试集
                               print(data['y'])
                               print(train['y'])
                               print(test['y'])
                               
![自助法](https://github.com/Desir10101000Dj/--/blob/master/image/%E8%87%AA%E5%8A%A9%E6%B3%95.PNG?raw=true)
                               
其中左边那一列是指数据数组编号，右边那一列的数字为‘1’是表示该编号下的数组元素被抽中，否则就没有被抽中。
###  2.2.4 调参与最终模型 ###
    调整算法参数和最终使用全部数据训练出最终模型。

## 2.3 性能度量 ##
    性能度量：衡量模型泛化能力的评价标准。

    回归任务最常用的性能度量时“均方误差”。

### 2.3.1 错误率与精度 ###
见2.1
### 2.3.2 查准率、查全率与F1 ###
    真正例TP、假正例FP、真反例TN、假反例FN

    可以组成混淆矩阵


                                                  分类结果混淆矩阵
                                                     预测结果
                                                真实情况	正例	反例
                                                
                                                正例	真正例TP	假反例FN
                                                
                                                反例	假正例FP	真反例TN
                                                
                                                查准率P = TP ／ TP+FP

                                                查全率R = TP ／ TP+FN

    查全和查准都时针对正例，查准率是预测为正例中正确的概率。查全率是正例中被预测为正例的概率。

    查准率-查全率曲线-P-R曲线-P-R图

    通过“平衡点”BEP 查准率=查全率时的取值，对比不同学习器的优劣。

    因为BEP过于简单，所以通常使用F1度量来评判学习器的好坏。

    F1 = (2 * P * R) / (P + R) = （2 * TP）／ (m + TP -TN)

    Fβ = ((1+β2)∗P∗R)  /   ((β2∗P)+R) 。β=1退化为标准F1度量，β<1查全率有更大影响，β>1查准率有更大影响。

    多个混淆矩阵综合考察时有两种方法。

    第一种，平均每个混淆矩阵的P，R，F的平均值，称为 “宏”

    第二种，平均TP、FP、TN、FN之后计算P、R、F，成为 “微”

  ### 2.3.3 ROC与AUC ###
    ROC:一般情况下泛化性能的好坏(均等代价错误率)。

    根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，分别以他们为横、纵坐标作图，就得到了ROC曲线。
    
    AUC：可通过对ROC曲线下各部分的面积求和而得。
    
### 2.3.4 代价敏感错误率与代价曲线  ###
    代价敏感错误率：为错误赋予‘非均等代价’。
    代价曲线：将ROC曲线上的每一点转换为代价平面上的一条线段，然后取所有线段的下界，得到代价曲线，围成的面积即为在所有条件下学习器的期望总体代价。

    



## 2.4比较检验 ##
    利用统计学的方法让我们进行学习器性能比较提供了重要依据。


### 2.4.1假设检验 ###
    二项检验：仅作一次或少量留出法此检验更适合。
    t检验：如果进行了多次留出法或者是交叉检验法时，用此方法。



### 2.4.2交叉验证t检验 ###
    针对不同学习器的性能比较。



### 2.4.3McNemar检验 ###
    假设两个学习器性能相同考虑变量Tx2。



### 2.4.4Friedman检验与Nemenyi后续检验 ###
     Friedman检验:针对多个算法的不同。
     Nemenyi后续检验：Friedman检验的“后续检验”。
    



## 2.5偏差与方差 ##
    偏差：期望输出与真实标记的差别称为偏差。
    方差：度量了同等大小的训练集的变动所导致的学习性能的变化。
     一般来说，方差和偏差是有冲突的。





## 2.6阅读材料 略 ##







## 习题 ##
 #### 1.数据集包含1000个样本，其中500个正例，500个反例，将其划分为包含70%样本的训练集和30%样本的测试集用于留出法评估，试估算共有多少种划分方式。
一个组合问题，从500500正反例中分别选出150150正反例用于留出法评估，所以可能取法应该是(C150500)2种。

#### 2.数据集包含100个样本，其中正反例各一半，假定学习算法所产生的模型是将新样本预测为训练样本数较多的类别（训练样本数相同时进行随机猜测），试给出用10折交叉验证法和留一法分别对错误率进行评估所得的结果。

10折交叉检验：由于每次训练样本中正反例数目一样，所以讲结果判断为正反例的概率也是一样的，所以错误率的期望是5050%。 
留一法：如果留下的是正例，训练样本中反例的数目比正例多一个，所以留出的样本会被判断是反例；同理，留出的是反例，则会被判断成正例，所以错误率是100100%。

#### 3.若学习器A的F1值比学习器B高，试析A的BEP值是否也比B高。
F1值的大小与BEP值并没有明确的关系。 

#### 4.试述真正例率（TPR）、假正例率（FPR）与查准率（P）、查全率（R）之间的联系。
查全率: 真实正例被预测为正例的比例 
真正例率: 真实正例被预测为正例的比例 
显然查全率与真正例率是相等的。 
查准率:预测为正例的实例中真实正例的比例 
假正例率: 真实反例被预测为正例的比例 
两者并没有直接的数值关系。

#### 5.试证明(2.22)AUC=1−lrankAUC=1−lrank
略

#### 6.试述错误率与ROC曲线之间的关系
ROCROC曲线每个点对应了一个TPRTPR与FPRFPR，此时对应了一个错误率。 
Ecost=(m+∗(1−TPR)∗cost01+m−∗FPR∗cost10)/(m++m−)Ecost=(m+∗(1−TPR)∗cost01+m−∗FPR∗cost10)/(m++m−) 
学习器会选择错误率最小的位置作为截断点。

#### 7.试证明任意一条ROC曲线都有一条代价曲线与之对应，反之亦然。
由定义可以知道TPR与FPR都是由0上升到1，那么FNR则是由1下降到0。 
每条ROC曲线都会对应一条代价曲线，由于第一条代价线段的是(0,0),(1,1)(0,0),(1,1)，最后是(0,1)(1,0)(0,1)(1,0), 
所有代价线段总会有一块公共区域，这个区域就是期望总体代价，而这块区域的边界就是代价曲线，且肯定从(0,0)(0,0)到(1,0)(1,0)。 
在有限个样本情况下，ROC是一条折线，此时根据代价曲线无法还原ROC曲线。但若是理论上有无限个样本，ROC是一条连续的折线，代价曲线也是连续的折线，每个点的切线可以求出TPR与FNR从而得到唯一的ROC曲线。(反证法)

#### 8.Min-Max规范化与z-score规范化如下所示。试析二者的优缺点。
略

#### 9.试述卡方检验过程。
略
#### 10.试述在使用FriedmanFriedman检验中使用式(2.34)与(2.35)的区别
略

        
