# 第二次读书笔记 #
---
## 1.1引言 ##
在引言中提了这样一个问题：人类能做出有效的判断，是因为我们已经积累了许多经验，而通过对经验的利用，就能对新情况做出有效的决策，那么这种对经验的利用计算机能帮忙吗？

书中给出了答案，在计算机系统中，“经验“通常以数据形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生”模型“的算法，即”学习算法“，有了这个算法，还有经验数据，我们就能在计算机上模拟出一个模型，用这个模型来解决或者预测一些东西。

我的理解：学习算法其实也算是一个模型，是一个演化的”干模型“，有了数据过后赋予它灵魂，这样便会产生一个解决实际问题的”模型“。

## 1.2基本术语 ##
我总结了几个主要术语
### 样本空间 ###
所有可能存在的、合理的、情况的集合。


### 假设空间 ###
假设空间是在已知属性和属性可能取值的情况下，对所有可能满足目标的情况的一种毫无遗漏的假设集合。


### 版本空间 ###
现实问题中，我们常面临很大的假设空间，但学习过程是根据有限的样本训练集进行的，那么对于不同版本的训练集，应该会有不同版本的“删除后”的假设空间与之对应。便称之为版本空间。 

### 泛化能力 ###
个人认为就是学习算法利用数据归纳成一个模型的能力


### 监督学习 ###
分类和回归的代表。



### 无监督学习 ###
聚类的代表。


### 独立同分布 ###

- [见第一次笔记](第一次笔记.md)
### 1.3假设空间 ###
归纳与演绎是科学推理的两大基本手段。前者是特殊到一般的”泛化”过程，后者则相反；显然，”从样例中学习“是一个归纳的过程，因此亦称”归纳学习“。

概念学习中最基本的是布尔概念学习，即对”是“”不是“这样的的可表示为0/1布尔值的目标概念的学习。（我觉得计算机学习方面无论在硬件还是在算法的研究过程中”0/1“这种思想是非常重要的）

作者将学习过程看做了一个在所有假设组成的空间（以判断西瓜的好坏为例子）中进行搜索的过程，搜索的目标是找到与训练集”匹配“的假设，即能够将训练集中的瓜判断正确的假设？

假设空间是在已知属性和属性可能取值的情况下，对所有可能满足目标的情况的一种毫无遗漏的假设集合。可以有许多策略对这个假设空间进行搜索，例如自下而上、从一般到特殊，或是自底而上、从一般到特殊（这个过程让我想到了递归和迭代这个思想，甚至是二叉树类似的树状数据结构，当然，肯定不是这么简单的）

##### 这里有一个问题在实际问题中可能有版本空间既然是与训练集一致的，那么是否版本空间越复杂越多就证明了样本数据的“独立”？ #####
我个人认为是的，一般情况下，数据的离散型越强，对于学习算法的要求就越高，但是所得出的模型就更好，这或许是个笨办法。

### 1.4归纳偏好 ###
机器学习算法在学习过程中对某种类型的假设的偏好称为”归纳偏好“或者简称为”偏好“（很容易理解，在现实生活过程中，我们遇到的许多问题都有其特殊的地方，正所谓”对症下药“，如果没有偏好的话和”死马当活马医“就没有区别了，为了避免这种盲目性，解决问题时，前期工作和准备是必须的，这样我们才能从一个比较好的”偏好“方向入手，效率才会高）

作者在后面提到了”奥卡姆剃刀“：奥卡姆剃刀定律（Occam's Razor, Ockham's Razor）又称“奥康的剃刀”，它是由14世纪英格兰的逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“简单有效原理”。正如他在《箴言书注》2卷15题说“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。”这个定律很美丽，完全符合科学研究的简洁性。但是在实际问题中有时候不得不走复杂的一步，科学正是这样极复杂又极其简单的事情，优雅耐人寻味。

作者为了说明归纳偏好的重要性，即一味的讨论什么算法更好，脱离实际是毫无意义的，且如果我们对要解决的问题一无所知且并假设其分布完全随机且平等，那么任何算法的预期性能都是相似的，提出了这个NFL定理的解释。

一开始我没有看懂于是我在csdn上找到了比较清楚的证明（用到了集合和概率统计的知识）

- [证明]（https://blog.csdn.net/u013238941/article/details/79091252）

### 1.5发展历程 ###
机器学习是人工智能研究发展的必然产物。

总的来看五十年代中后期，基于神经网路的”连接主义“和现在盛行的深度学习研究有着异曲同工之妙。

”机械学习”（死记硬背学习） “示教学习” “类比学习” “归纳学习” 现在研究最广的就是”归纳学习“，即从样例中学习。“归纳学习”一直都是研究的重点方法和方向


### 1.6应用现状 ###

作者写了大概三类
#### 计算机视觉 ####
#### 数据分析（数据挖掘）####
#### 自然语言处理 ####

### 1.7阅读材料（略） ###

### 习题 ###
#### 1.1 版本空间 ####
答：一共有7种。 
因为所有的好瓜坏瓜分别只有一种，所以可以由最特殊的逐步“泛化”，只要不是全部为*就不会出错。同时存在好瓜的样本，则一定不是空集。 
1. 青绿^蜷曲^浊响=好瓜 
2. *^蜷曲^浊响=好瓜 
3. 青绿^ * ^浊响=好瓜 
4. 青绿^蜷曲^ * =好瓜 
5. 青绿^ * ^ * =好瓜 
6. * ^蜷曲^ * =好瓜 
7. * ^ * ^浊响=好瓜

#### 1.2估算假设空间大小 ####
答：首先单个合取式有3*4*4=49种；

这里要说明一下，数据是完全按照表1.1进行计算，在表1.1中，色泽只有青绿和乌黑，不考虑下文中作者的假设中包含的浅白。如果考虑浅白，则这里答案4*4*4+1=65种。 
在不考虑任何冗余的情况下，是一个简单的组合问题，从49中选择1/2/3/…/k个的组合之和。 
考虑冗余就有点难度了，这里就不考虑了

#### 1.3偏好设计 ####
首先有可能不存在与所有训练样本都一致的假设，也就意味着出现了特征完全一样但是标记不一样。具体来说就是，只要没有出现【特征一样标记不一样】的情况，最少能找到一个唯一的限定的假设。
下面是我的一些偏好： 
 1. 其属性重复次数出现最多的就是好瓜
 2. 其属性重复次数出现最少的就是好瓜
 3. 三个属性值都没有重复过的是好瓜

#### 1.4证明 ####
之前是严格的想不想等即1和0的问题，如果h(x)=f(x)则认为没有误差，一旦不相等则记为一次误差。这里用l()函数，可以认为l是一个度量h(x)与f(x)之间差距的函数。证明过程略了，数学功底不够。

#### 1.5机器学习在互联网搜索哪些环节起作用 #####
其实互联网搜索从广义上讲必需用到机器学习的方法
预测输入，输入匹配，网页匹配度，智能抓取，预加载，网页排序（网上找的等等）















