  # 第五次读书笔记 #
  ---
  
  
  ## 4.1基本流程 ##
组成：一个根节点、若干内部节点和若干叶节点；叶节点对应了不同的预测结果，我们的目的是从包含样本全集的根节点找到它到每个叶节点的路径对应一个判定预测序列。 
目的：产生一棵泛化能力强的决策树。什么叫做泛化能力强呢？就是这颗决策树能够对之前没见过的样本也能做出正确的预测。整个过程如下

输入：训练集D，属性集A
1、生成节点node
2、如果 D中的样本全部属于同意类别C，那么就将node标记为C类的叶节点。返回。
3、如果2不满足，并且属性集为空或者D样本在A上的取值相同，就将node标记为叶节点并且类别设置为D中样本最多的类；返回。
4、一般情况：从A中选择最优的属性划分A'，根据D中在A'上的属性可能取值进行划分，划分完之后属性集大小减1，样本D在当前节点上分为N（N为D在A'属性的N种取值）。
输出：以node位根节点的一棵决策树。


那么，显然能发现三种导致递归返回的情况： 
1、当前节点所包含的样本全部属于同一类，无需划分 
2、属性集为空或者数据集在当前属性集上所有取值相同，无法划分 
3、当前节点所包含的样本集合为空，不能划分。

## 4.2划分选择
      1、信息增益
      利用熵降的大小来进行判断。

      2、增益率
      很显然，对离散数据增益可以有不错的划分效果。但是如果变成连续数据呢？假设样本某个连续特征的取值各不相同，那么只要选取这一特征的增益率一定是最大的。那么在增益的基础上除以一个IV（a）就是增益率。 
      IV（a）是什么呢？a是属性的取值可能性，可能性越多IV（a）越大。

      3.基尼指数
      直观来看，基尼指数就是反映从数据集中随机抽取两个样本，其类别标记不一致的概率。

## 4.3剪枝处理
基本策略：预剪枝、后剪枝。 
预剪枝是在决策树生成过程中，如果这个节点进行划分，不能带来泛化性能的提升，则停止划分并将该节点设置为叶子节点。 
后剪枝则是先训练好一棵树，然后自底向上对非叶子节点进行考察，如果将该节点对应的子树替换为叶节点能不能带来泛化性能的提升，能就将该子树替换为叶节点。 
如何判断泛化性能的提升？将训练集分出一部分当验证集。

### 4.3.1 预剪枝
在每次决定用一个特征去划分数据集的时候，都用验证集的数据去判断这次划分是否能提高验证集的精度。 
优点：预剪枝使得决策树很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少来决策树的训练时间开销和测试时间开销。 
缺点：虽然某次划分可能并不能带来性能上的提升，但是后序的划分可能会带来性能上的提升；预剪枝基于“贪心”的本质给决策树带来欠拟合的风险。

### 4.3.2后剪枝
后剪枝就是从某个节点后续划分去掉，将该节点设置为叶节点时，如果性能得到提升则进行剪枝操作。从性能上后剪枝一般比预剪枝的性能要更好，但是其时间开销更大。
