  # 第五次读书笔记 #
  ---
  
  
  ## 4.1基本流程 ##
组成：一个根节点、若干内部节点和若干叶节点；叶节点对应了不同的预测结果，我们的目的是从包含样本全集的根节点找到它到每个叶节点的路径对应一个判定预测序列。 
目的：产生一棵泛化能力强的决策树。什么叫做泛化能力强呢？就是这颗决策树能够对之前没见过的样本也能做出正确的预测。整个过程如下

输入：训练集D，属性集A
1、生成节点node
2、如果 D中的样本全部属于同意类别C，那么就将node标记为C类的叶节点。返回。
3、如果2不满足，并且属性集为空或者D样本在A上的取值相同，就将node标记为叶节点并且类别设置为D中样本最多的类；返回。
4、一般情况：从A中选择最优的属性划分A'，根据D中在A'上的属性可能取值进行划分，划分完之后属性集大小减1，样本D在当前节点上分为N（N为D在A'属性的N种取值）。
输出：以node位根节点的一棵决策树。


那么，显然能发现三种导致递归返回的情况： 
1、当前节点所包含的样本全部属于同一类，无需划分 
2、属性集为空或者数据集在当前属性集上所有取值相同，无法划分 
3、当前节点所包含的样本集合为空，不能划分。

## 4.2划分选择
      1、信息增益
      利用熵降的大小来进行判断。

      2、增益率
      很显然，对离散数据增益可以有不错的划分效果。但是如果变成连续数据呢？假设样本某个连续特征的取值各不相同，那么只要选取这一特征的增益率一定是最大的。那么在增益的基础上除以一个IV（a）就是增益率。 
      IV（a）是什么呢？a是属性的取值可能性，可能性越多IV（a）越大。

      3.基尼指数
      直观来看，基尼指数就是反映从数据集中随机抽取两个样本，其类别标记不一致的概率。

## 4.3剪枝处理
基本策略：预剪枝、后剪枝。 
预剪枝是在决策树生成过程中，如果这个节点进行划分，不能带来泛化性能的提升，则停止划分并将该节点设置为叶子节点。 
后剪枝则是先训练好一棵树，然后自底向上对非叶子节点进行考察，如果将该节点对应的子树替换为叶节点能不能带来泛化性能的提升，能就将该子树替换为叶节点。 
如何判断泛化性能的提升？将训练集分出一部分当验证集。

### 4.3.1 预剪枝
在每次决定用一个特征去划分数据集的时候，都用验证集的数据去判断这次划分是否能提高验证集的精度。 
优点：预剪枝使得决策树很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少来决策树的训练时间开销和测试时间开销。 
缺点：虽然某次划分可能并不能带来性能上的提升，但是后序的划分可能会带来性能上的提升；预剪枝基于“贪心”的本质给决策树带来欠拟合的风险。

### 4.3.2后剪枝
后剪枝就是从某个节点后续划分去掉，将该节点设置为叶节点时，如果性能得到提升则进行剪枝操作。从性能上后剪枝一般比预剪枝的性能要更好，但是其时间开销更大。

### 习题

 决策树算法
 
 main.py
 
 import json
import math
import plotTree
import numpy as np
import pandas as pd

D_keys = {
    '色泽': ['青绿', '乌黑', '浅白'],
    '根蒂': ['蜷缩', '硬挺', '稍蜷'],
    '敲声': ['清脆', '沉闷', '浊响'],
    '纹理': ['稍糊', '模糊', '清晰'],
    '脐部': ['凹陷', '稍凹', '平坦'],
    '触感': ['软粘', '硬滑'],
}
keys = ['是', '否']


# 读取数据
def loadData(filename):
    dataSet = pd.read_csv(filename)
    return dataSet


# 叶节点选择其类别为D中样本最多的类
def choose_largest_example(D):
    count = D['好瓜'].value_counts()
    return '是' if count['是'] > count['否'] else '否'


# 判断D中的样本在A上的取值是否相同
def same_value(D, A):
    for key in A:
        if key in D_keys and len(D[key].value_counts()) > 1:
            return False

    return True


# 计算给定数据集的熵
def calc_Ent(dataSet):
    numEntries = dataSet['power'].sum()
    Count = dataSet.groupby('好瓜')['power'].sum()
    Ent = 0.0

    for key in keys:
        # print(Count[key])
        if key not in Count:
            Ent -= 0.0
        else:
            prob = Count[key] / numEntries
            Ent -= prob * math.log(prob, 2)

    return Ent


# 计算按key划分的信息增益值
def calc_Gain_D(D, D_no_nan, key, Ent_D):
    Ent = 0.0
    D_size = D['power'].sum()
    D_nan_size = D_no_nan['power'].sum()
    for value in D_keys[key]:
        Dv = D.loc[D[key] == value]
        Dv_size = Dv['power'].sum()
        Ent_Dv = calc_Ent(Dv)
        Ent += Dv_size / D_nan_size * Ent_Dv

    return D_nan_size / D_size * (Ent_D - Ent)


# 生成连续值属性的候选划分点集合T
def candidate_T(D, key, n):
    L = set(D[key])
    T = []
    a, Sum = 0, 0
    for value in L:
        Sum += value
        a += 1
        if a == n:
            T.append(Sum / n)
            a, Sum = 0, 0

    if a > 0:
        T.append(Sum / a)

    return T


# 计算样本D基于划分点t二分后的连续值属性信息增益
def calc_Gain_t(D, D_no_nan, key, t, Ent_D):
    Ent = 0.0
    D_size = D['power'].sum()
    D_nan_size = D_no_nan['power'].sum()

    Dv = D.loc[D[key] <= t]
    Dv_size = Dv['power'].sum()
    Ent_Dv = calc_Ent(Dv)
    Ent += Dv_size / D_nan_size * Ent_Dv

    Dv = D.loc[D[key] > t]
    Dv_size = Dv['power'].sum()
    Ent_Dv = calc_Ent(Dv)
    Ent += Dv_size / D_nan_size * Ent_Dv

    return D_nan_size / D_size * (Ent_D - Ent)


# 计算样本D基于不同划分点t二分后的连续值属性信息增益，找出最大增益划分点
def calc_Gain_C(D, D_no_nan, key, Ent_D):
    n = 2
    T = candidate_T(D, key, n)
    max_Gain, max_partition = -1, -1
    for t in T:
        Gain = calc_Gain_t(D, D_no_nan, key, t, Ent_D)
        if max_Gain < Gain:
            max_Gain = Gain
            max_partition = t

    return max_Gain, max_partition


# 从A中选择最优的划分属性值，若为连续值，返回划分点
def choose_best_attribute(D, A):
    max_Gain, max_partition, partition, best_attr = -1, -1, -1, ''
    for key in A:
        # 划分属性为离散属性时
        if key in D_keys:
            D_no_nan = D.loc[pd.notna(D[key])]
            Ent_D = calc_Ent(D_no_nan)
            Gain = calc_Gain_D(D, D_no_nan, key, Ent_D)
        # 划分属性为连续属性时
        else:
            D_no_nan = D.loc[pd.notna(D[key])]
            Ent_D = calc_Ent(D_no_nan)
            Gain, partition = calc_Gain_C(D, D_no_nan, key, Ent_D)

        if max_Gain < Gain:
            best_attr = key
            max_Gain = Gain
            max_partition = partition

    return best_attr, max_partition


# 函数TreeGenerate 递归生成决策树，以下情形导致递归返回
# 1. 当前结点包含的样本全属于一个类别
# 2. 当前属性值为空， 或是所有样本在所有属性值上取值相同，无法划分
# 3. 当前结点包含的样本集合为空，不可划分
def TreeGenerate(D, A):
    Count = D['好瓜'].value_counts()
    if len(Count) == 1:
        return D['好瓜'].values[0]

    if len(A) == 0 or same_value(D, A):
        return choose_largest_example(D)

    node = {}
    best_attr, partition = choose_best_attribute(D, A)
    D_size = D.shape[0]
    # 最优划分属性为离散属性时
    if best_attr in D_keys:
        for value in D_keys[best_attr]:
            Dv = D.loc[D[best_attr] == value].copy()
            Dv_size = Dv.shape[0]
            Dv.loc[pd.isna(Dv[best_attr]), 'power'] = Dv_size / D_size
            if Dv.shape[0] == 0:
                node[value] = choose_largest_example(D)
            else:
                new_A = [key for key in A if key != best_attr]
                node[value] = TreeGenerate(Dv, new_A)

    # 最优划分属性为连续属性时
    else:
        # print(best_attr, partition)
        # print(D.values)
        left = D.loc[D[best_attr] <= partition].copy()
        Dv_size = left.shape[0]
        left.loc[pd.isna(left[best_attr]), 'power'] = Dv_size / D_size
        left_key = '<= ' + str(partition)

        if left.shape[0] == 0:
            node[left_key] = choose_largest_example(D)
        else:
            node[left_key] = TreeGenerate(left, A)

        right = D.loc[D[best_attr] > partition].copy()
        Dv_size = right.shape[0]
        right.loc[pd.isna(right[best_attr]), 'power'] = Dv_size / D_size

        right_key = '> ' + str(partition)
        if right.shape[0] == 0:
            node[right_key] = choose_largest_example(D)
        else:
            node[right_key] = TreeGenerate(right, A)
    # plotTree.plotTree(Tree)
    return {best_attr: node}


# 获得下一层子树分支
def get_next_Tree(Tree, key, value):
    if key not in D_keys:
        partition = float(list(Tree[key].keys())[0].split(' ')[1])
        if value <= partition:
            value = '<= ' + str(partition)
        else:
            value = '> ' + str(partition)

    return Tree[key][value]


# 深度优先遍历，判断预测值
def dfs_Tree(Tree, row):
    if type(Tree).__name__ == 'dict':
        key = list(Tree.keys())[0]
        value = row[key]
        if pd.isnull(value):
            result = {key: 0 for key in D_keys['好瓜']}
            for next_key in Tree[key]:
                next_Tree = Tree[key][next_key]
                temp = dfs_Tree(next_Tree, row)
                result[temp] += 1

            return '是' if count['是'] > count['否'] else '否'

        else:
            next_Tree = get_next_Tree(Tree, key, value)
            return dfs_Tree(next_Tree, row)
    else:
        return Tree


# 测试决策树的准确率
def test_Tree(Tree, data_test):
    accuracy = 0
    for index, row in data_test.iterrows():
        result = dfs_Tree(Tree, row)
        if result == row['好瓜']:
            # print(row.values, Tree)
            accuracy += 1

    print('Hit:', accuracy, '/', data_test.shape[0])
    print('Accuracy:', accuracy / data_test.shape[0])


if __name__ == '__main__':
    # 读取数据
    filename = 'data_3.txt'
    dataSet = loadData(filename)
    dataSet.drop(columns=['编号'], inplace=True)
    # 考虑缺失值
    dataSet['power'] = 1.0

    index_train = [0, 1, 2, 5, 6, 9, 13, 14, 15, 16]
    data_train = dataSet.iloc[index_train]
    data_test = dataSet.drop(index_train)

    # 决策树训练
    A = [column for column in data_train.columns if column != '好瓜']
    Tree = TreeGenerate(data_train, A)

    # 决策树测试
    test_Tree(Tree, data_test)

    print(Tree)
    plotTree.createPlot(Tree)

ploTree.py



      import matplotlib.pyplot as plt
      from pylab import *

      # 定义文本框和箭头格式
      decisionNode = dict(boxstyle="sawtooth", fc="0.8")
      leafNode = dict(boxstyle="round4", fc="0.8")
      arrow_args = dict(arrowstyle="<-")
      mpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体
      mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题


      def plotMidText(cntrPt, parentPt, txtString):
          xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]
          yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]
          createPlot.ax1.text(xMid, yMid, txtString)


      def plotNode(nodeTxt, centerPt, parentPt, nodeType):  # 绘制带箭头的注解
          createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords="axes fraction", xytext=centerPt, textcoords="axes fraction",
                                  va="center", ha="center", bbox=nodeType, arrowprops=arrow_args)


      def getNumLeafs(myTree):  # 获取叶节点的数目
          numLeafs = 0
          firstStr = list(myTree.keys())[0]
          secondDict = myTree[firstStr]
          for key in secondDict.keys():
              if type(secondDict[key]).__name__ == 'dict':
                  numLeafs += getNumLeafs(secondDict[key])
              else:
                  numLeafs += 1
          return numLeafs


      def getTreeDepth(myTree):  # 获取树的层数
          maxDepth = 0
          firstStr = list(myTree.keys())[0]
          secondDict = myTree[firstStr]
          for key in secondDict.keys():
              if type(secondDict[key]).__name__ == 'dict':
                  thisDepth = 1 + getTreeDepth(secondDict[key])
              else:
                  thisDepth = 1
              if thisDepth > maxDepth: maxDepth = thisDepth
          return maxDepth


      def retrieveTree(i):  # 获取预定义的树
          listOfTrees = [{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},
                         {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}
                         ]
          return listOfTrees[i]


      def plotTree(myTree, parentPt, nodeTxt):
          numLeafs = getNumLeafs(myTree)
          getTreeDepth(myTree)
          firstStr = list(myTree.keys())[0]
          cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, \
                    plotTree.yOff)
          plotMidText(cntrPt, parentPt, nodeTxt)
          plotNode(firstStr, cntrPt, parentPt, decisionNode)
          secondDict = myTree[firstStr]
          plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD
          for key in secondDict.keys():
              if type(secondDict[key]).__name__ == 'dict':
                  plotTree(secondDict[key], cntrPt, str(key))
              else:
                  plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW
                  plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff),
                           cntrPt, leafNode)
                  plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
          plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD


      def createPlot(inTree):
          fig = plt.figure(1, figsize=(600, 30), facecolor='white')
          fig.clf()
          axprops = dict(xticks=[], yticks=[])
          createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)
          plotTree.totalW = float(getNumLeafs(inTree))
          plotTree.totalD = float(getTreeDepth(inTree))
          plotTree.xOff = -0.5 / plotTree.totalW;
          plotTree.yOff = 1.0;
          plotTree(inTree, (0.5, 1.0), '')
          plt.show()

![图片1.png](https://github.com/Desir10101000Dj/--/blob/master/image/%E5%9B%BE%E7%89%871.png)






